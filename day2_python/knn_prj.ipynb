{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Nearest Neighbor (kNN) mini-Project\n",
    "\n",
    "#### This jupyter notebook mini-Project exercise is a simplified version modified from Assignment 1 of *Stanford CS class CS231n: Convolutional Neural Networks for Visual Recognition* by changing the input data from the CIFAR10 dataset to the MNIST dataset.. For more details see the [course assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.\n",
    "\n",
    "The kNN classifier consists of two stages:\n",
    "\n",
    "- During training, the classifier takes the training data and simply remembers it\n",
    "- During testing, kNN classifies every test image by comparing to all training images and transfering the labels of the k most similar training examples\n",
    "- The value of k is cross-validated\n",
    "\n",
    "In this simplified exercise you will learn how to write efficient, vectorized code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run some setup code for this notebook.\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "# This is a bit of magic to make matplotlib figures appear inline in the notebook\n",
    "# rather than in a new window.\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from http://deeplearning.net/data/mnist/mnist.pkl.gz\n",
    "import urllib, os\n",
    "str_dataset = 'mnist.pkl.gz'\n",
    "if not os.path.isfile(str_dataset):\n",
    "    print('Downloading mnist dataset...')\n",
    "    url = 'http://deeplearning.net/data/mnist/mnist.pkl.gz'\n",
    "    urllib.urlretrieve(url, str_dataset)\n",
    "    print('Download complete')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset to traing and test sets\n",
    "import sys, gzip\n",
    "f = gzip.open(str_dataset, 'rb')\n",
    "if sys.version_info[0] < 3:\n",
    "    print(\"using python 2\")\n",
    "    import cPickle\n",
    "    train_set, valid_set, test_set = cPickle.load(f)\n",
    "else:\n",
    "    print(\"using python 3\")\n",
    "    import _pickle as cPickle\n",
    "    train_set, valid_set, test_set = cPickle.load(f,encoding='latin1')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_set[0].shape,valid_set[0].shape, test_set[0].shape)\n",
    "print(train_set[1].shape,valid_set[1].shape, test_set[1].shape)\n",
    "X_train, y_train, X_test, y_test = train_set[0], train_set[1], test_set[0], test_set[1]\n",
    "\n",
    "# f = np.load('cs231n/datasets/mnist.npz')\n",
    "# X_train, y_train = f['x_train'], f['y_train']\n",
    "# X_test, y_test = f['x_test'], f['y_test']\n",
    "\n",
    "print('Training data shape: ', X_train.shape)\n",
    "print('Training labels shape: ', y_train.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(train_set[0][i].reshape((28, 28))) #, cmap=cm.gray, interpolation='none')\n",
    "# Visualize some examples from the dataset.\n",
    "# We show a few examples of training images from each class.\n",
    "classes = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "num_classes = len(classes)\n",
    "samples_per_class = 7\n",
    "plt.figure(figsize=(10,10))\n",
    "for y, cls in enumerate(classes):\n",
    "    idxs = np.flatnonzero(y_train == y)\n",
    "    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
    "    for i, idx in enumerate(idxs):\n",
    "        plt_idx = i * num_classes + y + 1\n",
    "        plt.subplot(samples_per_class, num_classes, plt_idx)\n",
    "        plt.imshow(X_train[idx].reshape((28, 28)))\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title(cls)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsample the data for more efficient code execution in this exercise\n",
    "num_training = 5000\n",
    "mask = list(range(num_training))\n",
    "X_train = X_train[mask]\n",
    "y_train = y_train[mask]\n",
    "\n",
    "num_test = 500\n",
    "mask = list(range(num_test))\n",
    "X_test = X_test[mask]\n",
    "y_test = y_test[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the image data into rows\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIXME: <font color='blue'>Complete the KNearestNeighbor class below. Concretely, you just need to complete the sections in the TODO parts</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would now like to classify the test data with the kNN classifier. Recall that we can break down this process into two steps: \n",
    "\n",
    "1. First we must compute the distances between all test examples and all train examples. \n",
    "2. Given these distances, for each test example we find the k nearest examples and have them vote for the label\n",
    "\n",
    "Lets begin with computing the distance matrix between all training and test examples. For example, if there are **Ntr** training examples and **Nte** test examples, this stage should result in a **Nte x Ntr** matrix where each element (i,j) is the distance between the i-th test and j-th train example.\n",
    "\n",
    "In the KNearestNeighbor class:\n",
    "\n",
    "1) implement the function `compute_distances_two_loops` that uses a (very inefficient) double loop over all pairs of (test, train) examples and computes the distance matrix one element at a time.\n",
    "\n",
    "2) implement the function `compute_distances_one_loop` that uses a single loop over all pairs of (test, train) examples and computes the distance matrix one element at a time.\n",
    "\n",
    "3) implement the function `compute_distances_no_loops` that uses no loop to compute the distance matrix one element at a time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNearestNeighbor(object):\n",
    "  \"\"\" a kNN classifier with L2 distance \"\"\"\n",
    "\n",
    "  def __init__(self):\n",
    "    pass\n",
    "\n",
    "  def train(self, X, y):\n",
    "    \"\"\"\n",
    "    Train the classifier. For k-nearest neighbors this is just \n",
    "    memorizing the training data.\n",
    "\n",
    "    Inputs:\n",
    "    - X: A numpy array of shape (num_train, D) containing the training data\n",
    "      consisting of num_train samples each of dimension D.\n",
    "    - y: A numpy array of shape (N,) containing the training labels, where\n",
    "         y[i] is the label for X[i].\n",
    "    \"\"\"\n",
    "    self.X_train = X\n",
    "    self.y_train = y\n",
    "    self.close_k = np.empty\n",
    "    \n",
    "  def predict(self, X, k=1, num_loops=0):\n",
    "    \"\"\"\n",
    "    Predict labels for test data using this classifier.\n",
    "\n",
    "    Inputs:\n",
    "    - X: A numpy array of shape (num_test, D) containing test data consisting\n",
    "         of num_test samples each of dimension D.\n",
    "    - k: The number of nearest neighbors that vote for the predicted labels.\n",
    "    - num_loops: Determines which implementation to use to compute distances\n",
    "      between training points and testing points.\n",
    "\n",
    "    Returns:\n",
    "    - y: A numpy array of shape (num_test,) containing predicted labels for the\n",
    "      test data, where y[i] is the predicted label for the test point X[i].  \n",
    "    \"\"\"\n",
    "    if num_loops == 0:\n",
    "      dists = self.compute_distances_no_loops(X)\n",
    "    elif num_loops == 1:\n",
    "      dists = self.compute_distances_one_loop(X)\n",
    "    elif num_loops == 2:\n",
    "      dists = self.compute_distances_two_loops(X)\n",
    "    else:\n",
    "      raise ValueError('Invalid value %d for num_loops' % num_loops)\n",
    "\n",
    "    return self.predict_labels(dists, k=k)\n",
    "\n",
    "  def compute_distances_two_loops(self, X):\n",
    "    \"\"\"\n",
    "    Compute the distance between each test point in X and each training point\n",
    "    in self.X_train using a nested loop over both the training data and the \n",
    "    test data.\n",
    "\n",
    "    Inputs:\n",
    "    - X: A numpy array of shape (num_test, D) containing test data.\n",
    "\n",
    "    Returns:\n",
    "    - dists: A numpy array of shape (num_test, num_train) where dists[i, j]\n",
    "      is the Euclidean distance between the ith test point and the jth training\n",
    "      point.\n",
    "    \"\"\"\n",
    "    num_test = X.shape[0]\n",
    "    num_train = self.X_train.shape[0]\n",
    "    dists = np.zeros((num_test, num_train))\n",
    "    for i in xrange(num_test):\n",
    "      for j in xrange(num_train):\n",
    "        #####################################################################\n",
    "        # TODO:                                                             #\n",
    "        # Compute the l2 distance between the ith test point and the jth    #\n",
    "        # training point, and store the result in dists[i, j]. You should   #\n",
    "        # not use a loop over dimension.                                    #\n",
    "        #####################################################################\n",
    "        dists[i,j]=np.sqrt(np.sum(np.square(X[i,:] - self.X_train[j,:])))\n",
    "        #pass\n",
    "        #####################################################################\n",
    "        #                       END OF YOUR CODE                            #\n",
    "        #####################################################################\n",
    "    return dists\n",
    "\n",
    "  def compute_distances_one_loop(self, X):\n",
    "    \"\"\"\n",
    "    Compute the distance between each test point in X and each training point\n",
    "    in self.X_train using a single loop over the test data.\n",
    "\n",
    "    Input / Output: Same as compute_distances_two_loops\n",
    "    \"\"\"\n",
    "    num_test = X.shape[0]\n",
    "    num_train = self.X_train.shape[0]\n",
    "    dists = np.zeros((num_test, num_train))\n",
    "    for i in xrange(num_test):\n",
    "      #######################################################################\n",
    "      # TODO:                                                               #\n",
    "      # Compute the l2 distance between the ith test point and all training #\n",
    "      # points, and store the result in dists[i, :].                        #\n",
    "      #######################################################################\n",
    "      dists[i]=np.sqrt(np.sum(np.square(X[i,:]-self.X_train),axis=1))\n",
    "      # pass\n",
    "      #######################################################################\n",
    "      #                         END OF YOUR CODE                            #\n",
    "      #######################################################################\n",
    "    return dists\n",
    "\n",
    "  def compute_distances_no_loops(self, X):\n",
    "    \"\"\"\n",
    "    Compute the distance between each test point in X and each training point\n",
    "    in self.X_train using no explicit loops.\n",
    "\n",
    "    Input / Output: Same as compute_distances_two_loops\n",
    "    \"\"\"\n",
    "    num_test = X.shape[0]\n",
    "    num_train = self.X_train.shape[0]\n",
    "    dists = np.zeros((num_test, num_train)) \n",
    "    #########################################################################\n",
    "    # TODO:                                                                 #\n",
    "    # Compute the l2 distance between all test points and all training      #\n",
    "    # points without using any explicit loops, and store the result in      #\n",
    "    # dists.                                                                #\n",
    "    #                                                                       #\n",
    "    # You should implement this function using only basic array operations; #\n",
    "    # in particular you should not use functions from scipy.                #\n",
    "    #                                                                       #\n",
    "    # HINT: Try to formulate the l2 distance using matrix multiplication    #\n",
    "    #       and two broadcast sums.                                         #\n",
    "    #########################################################################\n",
    "    teX2=np.sum(X**2,axis=1).reshape(num_test,-1)\n",
    "    trX2=np.sum(self.X_train**2,axis=1)\n",
    "    trte2=np.dot(X,self.X_train.T)\n",
    "    dists=np.sqrt(teX2+trX2-2.0*trte2)\n",
    "    #pass\n",
    "    #########################################################################\n",
    "    #                         END OF YOUR CODE                              #\n",
    "    #########################################################################\n",
    "    return dists\n",
    "\n",
    "  def predict_labels(self, dists, k=1):\n",
    "    \"\"\"\n",
    "    Given a matrix of distances between test points and training points,\n",
    "    predict a label for each test point.\n",
    "\n",
    "    Inputs:\n",
    "    - dists: A numpy array of shape (num_test, num_train) where dists[i, j]\n",
    "      gives the distance betwen the ith test point and the jth training point.\n",
    "\n",
    "    Returns:\n",
    "    - y_pred: A numpy array of shape (num_test,) containing predicted labels for the\n",
    "      test data, where y[i] is the predicted label for the test point X[i].  \n",
    "    - close_k: A list of numpy array containing the k nearest neighbor of the test data\n",
    "    \"\"\"\n",
    "    num_test = dists.shape[0]\n",
    "    y_pred = np.zeros(num_test,dtype=int)\n",
    "    close_k = np.zeros((num_test,k),dtype=int)\n",
    "    for i in xrange(num_test):\n",
    "      # A list of length k storing the labels of the k nearest neighbors to\n",
    "      # the ith test point.\n",
    "      closest_y = []\n",
    "      closest_y = np.argsort(dists[i,:])[:k]\n",
    "      close_k[i] = closest_y\n",
    "      y_pred[i] = np.bincount(self.y_train[closest_y]).argmax()\n",
    "      #pass\n",
    "      #########################################################################\n",
    "      #                           END OF YOUR CODE                            # \n",
    "      #########################################################################\n",
    "\n",
    "    return y_pred, close_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a kNN classifier instance. \n",
    "# Remember that training a kNN classifier is a noop: \n",
    "# the Classifier simply remembers the data and does no further processing \n",
    "classifier = KNearestNeighbor()\n",
    "classifier.train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open cs231n/classifiers/k_nearest_neighbor.py and implement\n",
    "# compute_distances_two_loops.\n",
    "\n",
    "# Test your implementation:\n",
    "dists = classifier.compute_distances_two_loops(X_test)\n",
    "print(dists.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can visualize the distance matrix: each row is a single test example and\n",
    "# its distances to training examples\n",
    "plt.imshow(dists, interpolation='none')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now implement the function predict_labels and run the code below:\n",
    "# We use k = 1 (which is Nearest Neighbor).\n",
    "y_test_pred,close_k = classifier.predict_labels(dists, k=1)\n",
    "\n",
    "# Compute and print the fraction of correctly predicted examples\n",
    "num_correct = np.sum(y_test_pred == y_test)\n",
    "accuracy = float(num_correct) / num_test\n",
    "print('Got %d / %d correct => accuracy: %f' % (num_correct, num_test, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should expect to see approximately `90%` accuracy. Now lets try out a larger `k`, say `k = 5`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred,close_k = classifier.predict_labels(dists, k=5)\n",
    "num_correct = np.sum(y_test_pred == y_test)\n",
    "accuracy = float(num_correct) / num_test\n",
    "print('Got %d / %d correct => accuracy: %f' % (num_correct, num_test, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should expect to see a slightly better performance than with `k = 1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets speed up distance matrix computation by using partial vectorization\n",
    "# with one loop. Implement the function compute_distances_one_loop and run the\n",
    "# code below:\n",
    "dists_one = classifier.compute_distances_one_loop(X_test)\n",
    "\n",
    "# To ensure that our vectorized implementation is correct, we make sure that it\n",
    "# agrees with the naive implementation. There are many ways to decide whether\n",
    "# two matrices are similar; one of the simplest is the Frobenius norm. In case\n",
    "# you haven't seen it before, the Frobenius norm of two matrices is the square\n",
    "# root of the squared sum of differences of all elements; in other words, reshape\n",
    "# the matrices into vectors and compute the Euclidean distance between them.\n",
    "difference = np.linalg.norm(dists - dists_one, ord='fro')\n",
    "print('Difference was: %f' % (difference, ))\n",
    "if difference < 0.001:\n",
    "    print('Good! The distance matrices are the same')\n",
    "else:\n",
    "    print('Uh-oh! The distance matrices are different')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now implement the fully vectorized version inside compute_distances_no_loops\n",
    "# and run the code\n",
    "dists_two = classifier.compute_distances_no_loops(X_test)\n",
    "\n",
    "# check that the distance matrix agrees with the one we computed before:\n",
    "difference = np.linalg.norm(dists - dists_two, ord='fro')\n",
    "print('Difference was: %f' % (difference, ))\n",
    "if difference < 0.001:\n",
    "    print('Good! The distance matrices are the same')\n",
    "else:\n",
    "    print('Uh-oh! The distance matrices are different')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compare how fast the implementations are\n",
    "def time_function(f, *args):\n",
    "    \"\"\"\n",
    "    Call a function f with args and return the time (in seconds) that it took to execute.\n",
    "    \"\"\"\n",
    "    import time\n",
    "    tic = time.time()\n",
    "    f(*args)\n",
    "    toc = time.time()\n",
    "    return toc - tic\n",
    "\n",
    "two_loop_time = time_function(classifier.compute_distances_two_loops, X_test)\n",
    "print('Two loop version took %f seconds' % two_loop_time)\n",
    "\n",
    "one_loop_time = time_function(classifier.compute_distances_one_loop, X_test)\n",
    "print('One loop version took %f seconds' % one_loop_time)\n",
    "\n",
    "no_loop_time = time_function(classifier.compute_distances_no_loops, X_test)\n",
    "print('No loop version took %f seconds' % no_loop_time)\n",
    "\n",
    "# you should see significantly faster performance with the fully vectorized implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation\n",
    "\n",
    "We have implemented the k-Nearest Neighbor classifier but we set the value k = 5 arbitrarily. We will now determine the best value of this hyperparameter with cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_folds = 5\n",
    "k_choices = [1, 3, 5, 8, 10, 20]\n",
    "\n",
    "X_train_folds = []\n",
    "y_train_folds = []\n",
    "# Split up the training data into folds. After splitting, X_train_folds and   \n",
    "# y_train_folds are lists of length num_folds, where               \n",
    "# y_train_folds[i] is the label vector for the points in X_train_folds[i].    \n",
    "X_train_folds = np.array_split(X_train, num_folds)\n",
    "y_train_folds = np.array_split(y_train, num_folds)\n",
    "\n",
    "# A dictionary holding the accuracies for different values of k that we find\n",
    "# when running cross-validation. After running cross-validation,\n",
    "# k_to_accuracies[k] should be a list of length num_folds giving the different\n",
    "# accuracy values that we found when using that value of k.\n",
    "k_to_accuracies = {}\n",
    "\n",
    "################################################################################\n",
    "# Perform k-fold cross validation to find the best value of k. For each        #\n",
    "# possible value of k, run the k-nearest-neighbor algorithm num_folds times,   #\n",
    "# where in each case you use all but one of the folds as training data and the #\n",
    "# last fold as a validation set. Store the accuracies for all fold and all     #\n",
    "# values of k in the k_to_accuracies dictionary.                               #\n",
    "################################################################################\n",
    "for k in k_choices:\n",
    "    k_to_accuracies[k]=[]\n",
    "    for f in range(num_folds):\n",
    "        x_trn = []\n",
    "        y_trn = []\n",
    "        trn_idc = []\n",
    "        for i in range(num_folds):\n",
    "            if (i!=f):\n",
    "                x_trn.append(X_train_folds[i])\n",
    "                y_trn.append(y_train_folds[i])\n",
    "        x_trn = np.stack(x_trn).reshape(-1,X_train.shape[1])\n",
    "        y_trn = np.stack(y_trn).reshape(x_trn.shape[0])\n",
    "\n",
    "        x_val = X_train_folds[f]\n",
    "        y_val = y_train_folds[f]\n",
    "\n",
    "        classifier.train(x_trn, y_trn)\n",
    "        dists = classifier.compute_distances_no_loops(x_val)\n",
    "        y_val_pred,close_k = classifier.predict_labels(dists,k)\n",
    "        num_val = y_val.shape[0]\n",
    "        num_corr = np.sum(y_val_pred == y_val)\n",
    "        acc = float(num_corr) / num_val\n",
    "        k_to_accuracies[k].append(acc)\n",
    "\n",
    "# Print out the computed accuracies\n",
    "for k in sorted(k_to_accuracies):\n",
    "    for accuracy in k_to_accuracies[k]:\n",
    "        print('k = %2d, accuracy = %f' % (k, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the raw observations\n",
    "for k in k_choices:\n",
    "    accuracies = k_to_accuracies[k]\n",
    "    plt.scatter([k] * len(accuracies), accuracies)\n",
    "\n",
    "# plot the trend line with error bars that correspond to standard deviation\n",
    "accuracies_mean = np.array([np.mean(v) for k,v in sorted(k_to_accuracies.items())])\n",
    "accuracies_std = np.array([np.std(v) for k,v in sorted(k_to_accuracies.items())])\n",
    "plt.errorbar(k_choices, accuracies_mean, yerr=accuracies_std)\n",
    "plt.title('Cross-validation on k')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Cross-validation accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the cross-validation results above, choose the best value for k,   \n",
    "# retrain the classifier using all the training data, and test it on the test\n",
    "# data. You should be able to get above 28% accuracy on the test data.\n",
    "best_k = 7\n",
    "\n",
    "classifier = KNearestNeighbor()\n",
    "classifier.train(X_train, y_train)\n",
    "y_test_pred,close_k = classifier.predict(X_test, k=best_k)\n",
    "\n",
    "# Compute and display the accuracy\n",
    "num_correct = np.sum(y_test_pred == y_test)\n",
    "accuracy = float(num_correct) / num_test\n",
    "print('Got %d / %d correct => accuracy: %f' % (num_correct, num_test, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot some samples of correctly labled test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsample = 5\n",
    "correct_indices = np.nonzero(y_test_pred == y_test)[0][:nsample]\n",
    "ii =1\n",
    "plt.figure(figsize=(12,12))\n",
    "for inc_idx in correct_indices:\n",
    "    # plot the test image\n",
    "    plt.subplot(nsample, best_k+1, ii)\n",
    "    plt.imshow(X_test[inc_idx].reshape((28, 28)))\n",
    "    plt.title(\"Test:{},Pred:{}\".format(y_test[inc_idx],y_test_pred[inc_idx]),color=\"red\")\n",
    "    plt.axis('off')\n",
    "    ii+=1\n",
    "    # plot the k neighbors\n",
    "    for idx_close in close_k[inc_idx]:\n",
    "        plt.subplot(nsample, best_k+1, ii)\n",
    "        ii+=1\n",
    "        plt.imshow(X_train[idx_close].reshape((28, 28)))\n",
    "        plt.title(\"Train: {}\".format(y_train[idx_close]),color=\"blue\")\n",
    "        plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot some samples of incorrectly labled test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsample = 5\n",
    "incorrect_indices = np.nonzero(y_test_pred != y_test)[0][:nsample]\n",
    "ii =1\n",
    "plt.figure(figsize=(12,12))\n",
    "for inc_idx in incorrect_indices:\n",
    "    # plot the test image\n",
    "    plt.subplot(nsample, best_k+1, ii)\n",
    "    plt.imshow(X_test[inc_idx].reshape((28, 28)))\n",
    "    plt.title(\"Test:{}, Pred:{}\".format(y_test[inc_idx],y_test_pred[inc_idx]),color=\"red\") # Pred {:d},fontsize=10\n",
    "    plt.axis('off')\n",
    "    ii+=1\n",
    "    # plot the k neighbors\n",
    "    for idx_close in close_k[inc_idx]:\n",
    "        plt.subplot(nsample, best_k+1, ii)\n",
    "        ii+=1\n",
    "        plt.imshow(X_train[idx_close].reshape((28, 28)))\n",
    "        plt.title(\"Train: {}\".format(y_train[idx_close]),color=\"blue\")\n",
    "        plt.axis('off')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
